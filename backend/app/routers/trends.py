"""
TrueTrend CN - çƒ­è¯ API è·¯ç”±
æä¾›çƒ­è¯æ’è¡Œã€ç”Ÿå‘½å‘¨æœŸæ•°æ®ç­‰æ¥å£
"""

from fastapi import APIRouter, HTTPException, Query
from typing import Optional, List
from datetime import datetime

from ..services.mock_generator import MockDataGenerator
from ..services.real_score import real_score_calculator
from ..models.schemas import (
    TrendResponse, 
    TrendItem, 
    LifecycleData, 
    LifecyclePoint,
    TimelineResponse,
    TimelineMonth,
    Platform,
    Sentiment
)

router = APIRouter(prefix="/api/trends", tags=["trends"])

# åˆå§‹åŒ–æ¨¡æ‹Ÿæ•°æ®ç”Ÿæˆå™¨
mock_generator = MockDataGenerator(year=2024)


def _convert_to_trend_item(trend_data: dict) -> TrendItem:
    """å°†åŸå§‹æ•°æ®è½¬æ¢ä¸º TrendItem æ¨¡å‹"""
    return TrendItem(
        keyword=trend_data["keyword"],
        platforms=[Platform(p) for p in trend_data["platforms"]],
        raw_heat_score=trend_data["raw_heat_score"],
        real_score=trend_data.get("real_score", 0),
        sentiment=Sentiment(trend_data["sentiment"]),
        first_seen=datetime.fromisoformat(trend_data["first_seen"]),
        peak_time=datetime.fromisoformat(trend_data["peak_time"]),
        last_seen=datetime.fromisoformat(trend_data["last_seen"]),
        is_marketing=trend_data.get("is_marketing", False),
        platform_count=trend_data["platform_count"]
    )


@router.get("", response_model=TrendResponse)
async def get_trends(
    limit: int = Query(default=50, ge=1, le=100, description="è¿”å›æ•°é‡é™åˆ¶"),
    include_marketing: bool = Query(default=False, description="æ˜¯å¦åŒ…å«è¥é”€å†…å®¹"),
    min_platforms: int = Query(default=1, ge=1, le=5, description="æœ€å°‘å‡ºç°å¹³å°æ•°")
):
    """
    è·å–å¹´åº¦çƒ­è¯æ’è¡Œæ¦œ
    
    è¿”å›ç»è¿‡ RealScore ç®—æ³•åŠ æƒåçš„çƒ­è¯åˆ—è¡¨ï¼Œé»˜è®¤è¿‡æ»¤è¥é”€å†…å®¹
    """
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    raw_trends = mock_generator.generate_all_trends()
    
    # åº”ç”¨ RealScore ç®—æ³•
    processed_trends = real_score_calculator.process_all_trends(raw_trends)
    
    # è¿‡æ»¤
    filtered = processed_trends
    
    if not include_marketing:
        filtered = [t for t in filtered if not t.get("is_marketing", False)]
    
    if min_platforms > 1:
        filtered = [t for t in filtered if t["platform_count"] >= min_platforms]
    
    # é™åˆ¶è¿”å›æ•°é‡
    filtered = filtered[:limit]
    
    # è½¬æ¢ä¸ºå“åº”æ¨¡å‹
    trend_items = [_convert_to_trend_item(t) for t in filtered]
    
    return TrendResponse(
        trends=trend_items,
        total_count=len(trend_items),
        generated_at=datetime.now()
    )


@router.get("/{keyword}/lifecycle", response_model=LifecycleData)
async def get_keyword_lifecycle(keyword: str):
    """
    è·å–æŸä¸ªçƒ­è¯çš„ç”Ÿå‘½å‘¨æœŸæ•°æ®
    
    è¿”å›ä»è¯ç”Ÿåˆ°æ¶ˆäº¡çš„å®Œæ•´çƒ­åº¦æ›²çº¿
    """
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    raw_trends = mock_generator.generate_all_trends()
    
    # æŸ¥æ‰¾æŒ‡å®šå…³é”®è¯
    target_trend = None
    for trend in raw_trends:
        if trend["keyword"] == keyword:
            target_trend = trend
            break
    
    if not target_trend:
        raise HTTPException(status_code=404, detail=f"çƒ­è¯ '{keyword}' æœªæ‰¾åˆ°")
    
    lifecycle_raw = target_trend.get("lifecycle_data", [])
    
    if not lifecycle_raw:
        raise HTTPException(status_code=404, detail=f"çƒ­è¯ '{keyword}' æ²¡æœ‰ç”Ÿå‘½å‘¨æœŸæ•°æ®")
    
    # èšåˆæ¯æ—¥æ•°æ® (è·¨å¹³å°)
    daily_heat = {}
    for point in lifecycle_raw:
        date = point["timestamp"][:10]  # åªå–æ—¥æœŸéƒ¨åˆ†
        if date not in daily_heat:
            daily_heat[date] = 0
        daily_heat[date] += point["heat_score"]
    
    # è½¬æ¢ä¸ºç”Ÿå‘½å‘¨æœŸç‚¹
    sorted_dates = sorted(daily_heat.keys())
    max_heat = max(daily_heat.values())
    max_date = [d for d, h in daily_heat.items() if h == max_heat][0]
    
    data_points = []
    for i, date in enumerate(sorted_dates):
        heat = daily_heat[date]
        
        # åˆ¤æ–­é˜¶æ®µ
        if i < len(sorted_dates) * 0.2:
            phase = "birth"
        elif date < max_date:
            phase = "rise"
        elif date == max_date:
            phase = "peak"
        elif i > len(sorted_dates) * 0.8:
            phase = "death"
        else:
            phase = "decline"
        
        data_points.append(LifecyclePoint(
            timestamp=datetime.fromisoformat(date),
            heat_score=heat,
            phase=phase
        ))
    
    return LifecycleData(
        keyword=keyword,
        data_points=data_points,
        birth_date=datetime.fromisoformat(sorted_dates[0]),
        peak_date=datetime.fromisoformat(max_date),
        death_date=datetime.fromisoformat(sorted_dates[-1]) if len(sorted_dates) > 1 else None,
        total_days=len(sorted_dates)
    )


@router.get("/timeline", response_model=TimelineResponse)
async def get_timeline(
    year: int = Query(default=2024, ge=2020, le=2030)
):
    """
    è·å–å¹´åº¦æ—¶é—´è½´æ•°æ®
    
    æŒ‰æœˆä»½åˆ†ç»„ï¼Œæ˜¾ç¤ºæ¯æœˆçš„çƒ­é—¨è¯é¢˜
    """
    # æ›´æ–°ç”Ÿæˆå™¨å¹´ä»½
    generator = MockDataGenerator(year=year)
    timeline_data = generator.generate_timeline_data()
    
    # åº”ç”¨ RealScore
    timeline_months = []
    
    for month_key in sorted(timeline_data.keys()):
        trends = timeline_data[month_key]
        processed = real_score_calculator.process_all_trends(trends)
        
        # è¿‡æ»¤è¥é”€å¹¶å–å‰ 5 å
        non_marketing = [t for t in processed if not t.get("is_marketing", False)][:5]
        
        trend_items = [_convert_to_trend_item(t) for t in non_marketing]
        
        timeline_months.append(TimelineMonth(
            month=month_key,
            top_trends=trend_items
        ))
    
    return TimelineResponse(
        timeline=timeline_months,
        year=year
    )


@router.get("/debug/score-breakdown")
async def get_score_breakdown(keyword: str):
    """
    è°ƒè¯•æ¥å£ï¼šæŸ¥çœ‹æŸä¸ªçƒ­è¯çš„åˆ†æ•°è®¡ç®—è¿‡ç¨‹
    """
    raw_trends = mock_generator.generate_all_trends()
    processed = real_score_calculator.process_all_trends(raw_trends)
    
    for trend in processed:
        if trend["keyword"] == keyword:
            return {
                "keyword": keyword,
                "raw_heat_score": trend["raw_heat_score"],
                "real_score": trend["real_score"],
                "breakdown": trend.get("score_breakdown", {}),
                "platforms": trend["platforms"],
                "is_marketing": trend.get("is_marketing", False)
            }
    
    raise HTTPException(status_code=404, detail=f"çƒ­è¯ '{keyword}' æœªæ‰¾åˆ°")


# ============================================================
# Phase 2: å®æ—¶æ•°æ®æ¥å£
# ============================================================

from ..services.fetchers import get_fetcher_manager
from ..services.sentiment_analyzer import get_sentiment_analyzer, SentimentType


@router.get("/live")
async def get_live_trends(
    limit: int = Query(default=30, ge=1, le=100, description="è¿”å›æ•°é‡é™åˆ¶"),
    platforms: Optional[str] = Query(default=None, description="æŒ‡å®šå¹³å°ï¼Œé€—å·åˆ†éš” (weibo,zhihu,bilibili)"),
    use_cache: bool = Query(default=True, description="æ˜¯å¦ä½¿ç”¨ç¼“å­˜ (5åˆ†é’Ÿæœ‰æ•ˆ)")
):
    """
    ğŸ”´ å®æ—¶è·å–å¤šå¹³å°çƒ­æœæ•°æ®
    
    ä»å¾®åšã€çŸ¥ä¹ã€Bç«™å®æ—¶çˆ¬å–çƒ­æœï¼Œåˆå¹¶ç›¸åŒçƒ­è¯ï¼Œåº”ç”¨æƒ…æ„Ÿåˆ†æ
    
    æ³¨æ„: é¦–æ¬¡è°ƒç”¨å¯èƒ½è¾ƒæ…¢ (éœ€è¦ç½‘ç»œè¯·æ±‚), åç»­ä½¿ç”¨ç¼“å­˜
    """
    try:
        manager = get_fetcher_manager()
        analyzer = get_sentiment_analyzer()
        
        # è·å–å¹¶åˆå¹¶æ•°æ®
        merged_data = await manager.fetch_and_merge(
            limit_per_platform=limit,
            use_cache=use_cache
        )
        
        # è¿‡æ»¤æŒ‡å®šå¹³å°
        if platforms:
            platform_list = [p.strip() for p in platforms.split(",")]
            merged_data = [
                item for item in merged_data
                if any(p in item["platforms"] for p in platform_list)
            ]
        
        # åº”ç”¨æƒ…æ„Ÿåˆ†æ
        for item in merged_data:
            sentiment_result = analyzer.analyze(item["keyword"])
            item["sentiment"] = sentiment_result.sentiment.value
            item["sentiment_confidence"] = sentiment_result.confidence
        
        # åº”ç”¨ RealScore ç®—æ³•
        for item in merged_data:
            # æ·»åŠ å¿…è¦å­—æ®µç”¨äº RealScore è®¡ç®—
            item["is_marketing"] = False  # å®æ—¶æ•°æ®é»˜è®¤éè¥é”€
            
            real_score = real_score_calculator.calculate_real_score(item)
            item["real_score"] = real_score
        
        # æŒ‰ RealScore æ’åº
        merged_data.sort(key=lambda x: x.get("real_score", 0), reverse=True)
        
        return {
            "source": "live",
            "trends": merged_data[:limit],
            "total_count": len(merged_data),
            "generated_at": datetime.now().isoformat(),
            "cache_used": use_cache,
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"å®æ—¶æ•°æ®è·å–å¤±è´¥: {str(e)}"
        )


@router.get("/live/{platform}")
async def get_live_platform_trends(
    platform: str,
    limit: int = Query(default=50, ge=1, le=100)
):
    """
    è·å–å•ä¸ªå¹³å°çš„å®æ—¶çƒ­æœ
    
    Args:
        platform: weibo / zhihu / bilibili
    """
    valid_platforms = ["weibo", "zhihu", "bilibili"]
    
    if platform not in valid_platforms:
        raise HTTPException(
            status_code=400,
            detail=f"æ— æ•ˆå¹³å°ï¼Œå¯é€‰: {', '.join(valid_platforms)}"
        )
    
    try:
        manager = get_fetcher_manager()
        analyzer = get_sentiment_analyzer()
        
        data = await manager.fetch_single_platform(platform, limit)
        
        # æ·»åŠ æƒ…æ„Ÿåˆ†æ
        for item in data:
            sentiment_result = analyzer.analyze(item["keyword"])
            item["sentiment"] = sentiment_result.sentiment.value
            item["sentiment_confidence"] = sentiment_result.confidence
        
        return {
            "platform": platform,
            "trends": data,
            "total_count": len(data),
            "generated_at": datetime.now().isoformat()
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å– {platform} æ•°æ®å¤±è´¥: {str(e)}"
        )


@router.post("/analyze-sentiment")
async def analyze_sentiment_api(texts: List[str]):
    """
    æƒ…æ„Ÿåˆ†æ API
    
    è¾“å…¥ä¸€ç»„æ–‡æœ¬ï¼Œè¿”å›æƒ…æ„Ÿåˆ†æç»“æœ
    """
    if not texts:
        raise HTTPException(status_code=400, detail="texts ä¸èƒ½ä¸ºç©º")
    
    if len(texts) > 100:
        raise HTTPException(status_code=400, detail="å•æ¬¡æœ€å¤šåˆ†æ 100 æ¡æ–‡æœ¬")
    
    analyzer = get_sentiment_analyzer()
    results = []
    
    for text in texts:
        result = analyzer.analyze(text)
        results.append({
            "text": text[:100],  # æˆªæ–­é•¿æ–‡æœ¬
            "sentiment": result.sentiment.value,
            "confidence": result.confidence,
            "method": result.method
        })
    
    return {
        "results": results,
        "total": len(results)
    }


# ============================================================
# Phase 3: è¯„è®ºé‡‡é›† API
# ============================================================

from ..services.crawlers import BilibiliCommentsCrawler, ZhihuCommentsCrawler, WeiboCommentsCrawler


@router.get("/comments/{keyword}")
async def get_keyword_comments(
    keyword: str,
    platform: str = Query(default="bilibili", description="å¹³å°: bilibili, zhihu, weibo"),
    video_limit: int = Query(default=5, ge=1, le=20, description="è§†é¢‘/é—®é¢˜/å¸–å­æ•°é‡"),
    comment_limit: int = Query(default=30, ge=1, le=100, description="æ¯é¡¹è¯„è®ºæ•°")
):
    """
    è·å–çƒ­è¯ç›¸å…³è¯„è®º
    
    æ”¯æŒ Bç«™ã€çŸ¥ä¹ã€å¾®åš ä¸‰ä¸ªå¹³å°
    """
    analyzer = get_sentiment_analyzer()
    
    try:
        if platform == "bilibili":
            crawler = BilibiliCommentsCrawler()
            try:
                result = await crawler.crawl_keyword_comments(
                    keyword, 
                    video_limit=video_limit, 
                    comment_limit_per_video=comment_limit
                )
            finally:
                await crawler.close()
            
            # å¯¹è¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†æ
            for video in result.get("videos", []):
                for comment in video.get("comments", []):
                    sentiment_result = analyzer.analyze(comment["content"])
                    comment["sentiment"] = sentiment_result.sentiment.value
                    comment["sentiment_score"] = sentiment_result.confidence
        
        elif platform == "zhihu":
            # è‡ªåŠ¨åŠ è½½æŒä¹…åŒ–çš„ Cookie
            cookie = None
            try:
                cookie = await get_browser_auth().get_cookies("zhihu")
                if cookie:
                    print("[ZhihuCrawler] å·²åŠ è½½æŒä¹…åŒ–çš„ç™»å½• Cookie")
            except Exception as e:
                print(f"[ZhihuCrawler] Cookie åŠ è½½å¤±è´¥: {e}")
            
            crawler = ZhihuCommentsCrawler(cookie=cookie)
            try:
                result = await crawler.crawl_keyword_comments(
                    keyword, 
                    question_limit=video_limit, 
                    answer_limit=3,
                    comment_limit=comment_limit
                )
            finally:
                await crawler.close()
            
            # å¯¹è¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†æ
            for question in result.get("questions", []):
                for answer in question.get("answers", []):
                    for comment in answer.get("comments", []):
                        sentiment_result = analyzer.analyze(comment["content"])
                        comment["sentiment"] = sentiment_result.sentiment.value
                        comment["sentiment_score"] = sentiment_result.confidence
        
        elif platform == "weibo":
            # è‡ªåŠ¨åŠ è½½æŒä¹…åŒ–çš„ Cookie
            cookie = None
            try:
                cookie = await get_browser_auth().get_cookies("weibo")
                if cookie:
                    print("[WeiboCrawler] å·²åŠ è½½æŒä¹…åŒ–çš„ç™»å½• Cookie")
            except Exception as e:
                print(f"[WeiboCrawler] Cookie åŠ è½½å¤±è´¥: {e}")
            
            crawler = WeiboCommentsCrawler(cookie=cookie)
            try:
                result = await crawler.crawl_keyword_comments(
                    keyword, 
                    post_limit=video_limit, 
                    comment_limit_per_post=comment_limit
                )
            finally:
                await crawler.close()
            
            # å¯¹è¯„è®ºè¿›è¡Œæƒ…æ„Ÿåˆ†æ
            for post in result.get("posts", []):
                for comment in post.get("comments", []):
                    sentiment_result = analyzer.analyze(comment["content"])
                    comment["sentiment"] = sentiment_result.sentiment.value
                    comment["sentiment_score"] = sentiment_result.confidence
        
        else:
            raise HTTPException(
                status_code=400,
                detail="æ”¯æŒçš„å¹³å°: bilibili, zhihu, weibo"
            )
        
        return result
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è¯„è®ºé‡‡é›†å¤±è´¥: {str(e)}"
        )


@router.get("/comments/{keyword}/sentiment")
async def get_comments_sentiment_stats(
    keyword: str,
    platform: str = Query(default="bilibili", description="å¹³å°: bilibili, zhihu")
):
    """
    è·å–çƒ­è¯è¯„è®ºçš„æƒ…æ„Ÿç»Ÿè®¡
    
    è¿”å›å„æƒ…æ„Ÿç±»åˆ«çš„æ•°é‡å’Œå æ¯”
    """
    analyzer = get_sentiment_analyzer()
    sentiment_counts = {"happy": 0, "sad": 0, "angry": 0, "neutral": 0}
    total = 0
    
    try:
        if platform == "bilibili":
            crawler = BilibiliCommentsCrawler()
            try:
                result = await crawler.crawl_keyword_comments(keyword, video_limit=5, comment_limit_per_video=30)
            finally:
                await crawler.close()
            
            for video in result.get("videos", []):
                for comment in video.get("comments", []):
                    sentiment_result = analyzer.analyze(comment["content"])
                    sentiment_counts[sentiment_result.sentiment.value] += 1
                    total += 1
        
        elif platform == "zhihu":
            crawler = ZhihuCommentsCrawler()
            try:
                result = await crawler.crawl_keyword_comments(keyword, question_limit=5, answer_limit=3, comment_limit=30)
            finally:
                await crawler.close()
            
            for question in result.get("questions", []):
                for answer in question.get("answers", []):
                    for comment in answer.get("comments", []):
                        sentiment_result = analyzer.analyze(comment["content"])
                        sentiment_counts[sentiment_result.sentiment.value] += 1
                        total += 1
        
        else:
            raise HTTPException(status_code=400, detail="æ”¯æŒçš„å¹³å°: bilibili, zhihu")
        
        # è®¡ç®—å æ¯”
        stats = {
            sentiment: {
                "count": count,
                "percentage": round(count / total * 100, 1) if total > 0 else 0
            }
            for sentiment, count in sentiment_counts.items()
        }
        
        return {
            "keyword": keyword,
            "platform": platform,
            "total_comments": total,
            "sentiment_distribution": stats,
            "dominant_sentiment": max(sentiment_counts, key=sentiment_counts.get) if total > 0 else "neutral"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç»Ÿè®¡å¤±è´¥: {str(e)}")


# ============================================================
# Phase 3.5: ç™»å½•è®¤è¯ API
# ============================================================

from ..services.browser_auth import get_browser_auth, PLAYWRIGHT_AVAILABLE


@router.get("/auth/status")
async def get_auth_status():
    """
    è·å–å„å¹³å°çš„ç™»å½•çŠ¶æ€
    
    è¿”å›å¾®åšå’ŒçŸ¥ä¹çš„ç™»å½•ä¿¡æ¯
    """
    if not PLAYWRIGHT_AVAILABLE:
        return {
            "error": "Playwright æœªå®‰è£…",
            "install_command": "pip install playwright && playwright install chromium"
        }
    
    auth = get_browser_auth()
    return {
        "platforms": auth.get_login_status(),
        "browser_data_dir": str(auth._get_state_path("").parent)
    }


@router.post("/auth/login/{platform}")
async def trigger_login(platform: str):
    """
    è§¦å‘æ‰«ç ç™»å½•
    
    æ³¨æ„: è¿™ä¼šåœ¨æœåŠ¡å™¨ç«¯å¼¹å‡ºæµè§ˆå™¨çª—å£
    ä»…é€‚ç”¨äºæœ¬åœ°å¼€å‘ç¯å¢ƒ
    """
    if platform not in ["weibo", "zhihu"]:
        raise HTTPException(status_code=400, detail="æ”¯æŒçš„å¹³å°: weibo, zhihu")
    
    if not PLAYWRIGHT_AVAILABLE:
        raise HTTPException(
            status_code=500, 
            detail="Playwright æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install playwright && playwright install chromium"
        )
    
    auth = get_browser_auth(headless=False)
    
    try:
        success = await auth.login_with_qr(platform, timeout=180)
        
        if success:
            return {
                "status": "success",
                "message": f"{platform} ç™»å½•æˆåŠŸ",
                "cookie_available": True
            }
        else:
            return {
                "status": "failed",
                "message": f"{platform} ç™»å½•è¶…æ—¶æˆ–å¤±è´¥",
                "cookie_available": False
            }
    finally:
        await auth.close()


@router.get("/auth/cookies/{platform}")
async def get_platform_cookies(platform: str):
    """
    è·å–å¹³å°çš„ Cookie (ç”¨äºè°ƒè¯•)
    """
    if platform not in ["weibo", "zhihu"]:
        raise HTTPException(status_code=400, detail="æ”¯æŒçš„å¹³å°: weibo, zhihu")
    
    auth = get_browser_auth()
    cookie = await auth.get_cookies(platform)
    
    if cookie:
        # åªè¿”å›éƒ¨åˆ† Cookie ç”¨äºç¡®è®¤
        preview = cookie[:100] + "..." if len(cookie) > 100 else cookie
        return {
            "platform": platform,
            "has_cookie": True,
            "cookie_preview": preview,
            "cookie_length": len(cookie)
        }
    else:
        return {
            "platform": platform,
            "has_cookie": False,
            "message": "æœªç™»å½•ï¼Œè¯·å…ˆè°ƒç”¨ POST /auth/login/{platform}"
        }


# ============================================================
# GitHub å­˜æ¡£ API (å†å²çƒ­æœæ•°æ®)
# ============================================================

@router.get("/yearly/{year}")
async def get_yearly_trends(
    year: int,
    limit: int = Query(default=100, ge=1, le=500, description="è¿”å›æ•°é‡é™åˆ¶")
):
    """
    è·å–å¹´åº¦çƒ­è¯æ¦œ
    
    æ•°æ®æ¥æº: GitHub justjavac/weibo-trending-hot-search
    å¯ç”¨èŒƒå›´: 2020-11-24 è‡³ä»Š
    """
    from ..services.github_archive import get_yearly_hot_words
    
    if year < 2020 or year > datetime.now().year:
        raise HTTPException(
            status_code=400,
            detail=f"å¯ç”¨å¹´ä»½: 2020 ~ {datetime.now().year}"
        )
    
    try:
        result = await get_yearly_hot_words(year, limit)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å¹´åº¦æ•°æ®å¤±è´¥: {str(e)}")


@router.get("/monthly/{year}/{month}")
async def get_monthly_trends(
    year: int,
    month: int,
    limit: int = Query(default=50, ge=1, le=200, description="è¿”å›æ•°é‡é™åˆ¶")
):
    """
    è·å–æœˆåº¦çƒ­è¯æ¦œ
    
    æ•°æ®æ¥æº: GitHub justjavac/weibo-trending-hot-search
    """
    from ..services.github_archive import get_monthly_hot_words
    
    if month < 1 or month > 12:
        raise HTTPException(status_code=400, detail="æœˆä»½å¿…é¡»åœ¨ 1-12 ä¹‹é—´")
    
    if year < 2020 or year > datetime.now().year:
        raise HTTPException(
            status_code=400,
            detail=f"å¯ç”¨å¹´ä»½: 2020 ~ {datetime.now().year}"
        )
    
    try:
        result = await get_monthly_hot_words(year, month, limit)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æœˆåº¦æ•°æ®å¤±è´¥: {str(e)}")


@router.get("/archive/{date}")
async def get_daily_archive(date: str):
    """
    è·å–å•æ—¥çƒ­æœå­˜æ¡£
    
    Args:
        date: æ—¥æœŸæ ¼å¼ YYYY-MM-DD
        
    æ•°æ®æ¥æº: GitHub justjavac/weibo-trending-hot-search
    """
    from ..services.github_archive import get_daily_archive as fetch_daily
    
    # éªŒè¯æ—¥æœŸæ ¼å¼
    try:
        parsed_date = datetime.strptime(date, "%Y-%m-%d")
    except ValueError:
        raise HTTPException(status_code=400, detail="æ—¥æœŸæ ¼å¼é”™è¯¯ï¼Œåº”ä¸º YYYY-MM-DD")
    
    # éªŒè¯æ—¥æœŸèŒƒå›´
    min_date = datetime(2020, 11, 24)
    if parsed_date < min_date:
        raise HTTPException(status_code=400, detail="æ•°æ®æœ€æ—©å¯è¿½æº¯è‡³ 2020-11-24")
    if parsed_date > datetime.now():
        raise HTTPException(status_code=400, detail="æ—¥æœŸä¸èƒ½è¶…è¿‡ä»Šå¤©")
    
    try:
        result = await fetch_daily(date)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å­˜æ¡£å¤±è´¥: {str(e)}")

